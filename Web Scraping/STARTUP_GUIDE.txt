# üåê Web Scraping Project - Complete Guide

## üìã Project Overview

This project provides a complete, production-ready web scraper for extracting product information (names, prices, ratings) from e-commerce websites and saving the data to CSV files.

### ‚ú® Key Features

- **Easy to Use**: Simple API for scraping any website
- **Flexible**: Configure for any e-commerce site with CSS selectors
- **Robust**: Comprehensive error handling and logging
- **CSV Export**: Automatic structured data export
- **Well Documented**: Extensive examples and guides
- **Tested**: Works with real websites like books.toscrape.com

---

## üìÅ Project Files

```
Web Scraping/
‚îú‚îÄ‚îÄ scraper.py                 # Main scraper module (DO NOT EDIT - core library)
‚îú‚îÄ‚îÄ examples.py                # Interactive examples with menu
‚îú‚îÄ‚îÄ quickstart.py              # Quick test script
‚îú‚îÄ‚îÄ test_scraper.py            # Testing script
‚îú‚îÄ‚îÄ website_configs.py          # Pre-configured website examples
‚îú‚îÄ‚îÄ CONFIGURATION_GUIDE.py     # Detailed configuration help
‚îú‚îÄ‚îÄ run.bat                    # Windows batch runner
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îú‚îÄ‚îÄ README.md                  # Full documentation
‚îî‚îÄ‚îÄ STARTUP_GUIDE.txt         # This file
```

---

## üöÄ Getting Started (3 Steps)

### Step 1: Activate Virtual Environment

**Windows (Command Prompt):**
```bash
.venv\Scripts\activate
```

**Windows (PowerShell):**
```powershell
.\.venv\Scripts\Activate.ps1
```

**Mac/Linux:**
```bash
source venv/bin/activate
```

### Step 2: Install Packages (if needed)

```bash
pip install -r requirements.txt
```

### Step 3: Run the Scraper

**Option A - Using batch file (Windows):**
```bash
run.bat
```

**Option B - Using Python directly:**
```bash
python quickstart.py          # Quick test
python examples.py            # Interactive menu
python test_scraper.py        # Run tests
```

---

## üí° Quick Usage

### Basic Example

```python
from scraper import scrape_website

url = "https://books.toscrape.com/"

selectors = {
    'container': 'article.product_pod',
    'name': 'h3 a',
    'price': 'p.price_color',
    'rating': 'p.star-rating'
}

# Run the scraper
csv_file = scrape_website(url, selectors, 'products.csv')
print(f"Saved to: {csv_file}")
```

### Using Pre-configured Website

```python
from scraper import scrape_website
from website_configs import get_config

config = get_config('books_toscrape')

csv_file = scrape_website(
    url=config['url'],
    selectors=config['selectors'],
    output_file='books.csv'
)
```

---

## üîç Finding CSS Selectors

### Method: Browser Inspector

1. **Open Developer Tools**: Press `F12` (Windows/Linux) or `Cmd+Option+I` (Mac)
2. **Right-click a product** ‚Üí Select "Inspect Element"
3. **Look at the HTML** to find class names:
   - Product container: `<article class="product_pod">`
   - Product name: `<h3><a>Name</a></h3>`
   - Price: `<p class="price_color">¬£XX.XX</p>`
   - Rating: `<p class="star-rating">Four</p>`

4. **Test in Console**:
   ```javascript
   document.querySelectorAll('article.product_pod')  // Should show products
   ```

### CSS Selector Examples

| Pattern | Meaning |
|---------|---------|
| `.classname` | By CSS class |
| `#idname` | By element ID |
| `tag.class` | Tag with class |
| `.parent .child` | Nested elements |
| `h3 a` | `<a>` inside `<h3>` |

---

## üìö Available Resources

### Main Files

| File | Purpose |
|------|---------|
| `scraper.py` | Core library (WebScraper, CSVExporter classes) |
| `examples.py` | Interactive examples menu |
| `quickstart.py` | One-click test with books.toscrape.com |
| `test_scraper.py` | Verify all functionality works |
| `website_configs.py` | Pre-configured examples for popular sites |
| `README.md` | Complete documentation |
| `CONFIGURATION_GUIDE.py` | Detailed selector finding guide |

### Documentation

1. **README.md** - Full feature documentation and API reference
2. **CONFIGURATION_GUIDE.py** - Step-by-step configuration help
3. **website_configs.py** - Ready-to-use site configurations
4. **examples.py** - Interactive examples with explanations

---

## üéØ Next Steps

### For Beginners

1. ‚úÖ Run `python quickstart.py` to test the scraper
2. ‚úÖ Open `books_products.csv` to see the output format
3. ‚úÖ Run `python examples.py` for interactive examples
4. ‚úÖ Read `CONFIGURATION_GUIDE.py` to understand selectors
5. ‚úÖ Try configuring your own website

### For Advanced Users

1. Check `website_configs.py` for complex examples
2. Implement multi-page scraping with delays
3. Add error handling and retry logic
4. Consider using Selenium for JavaScript-heavy sites
5. Implement proxy rotation for large-scale scraping

---

## üìä Output Format

The CSV file contains:

```csv
name,price,rating
"A Light in the Attic","¬£51.77","Three"
"Twig","¬£23.19","One"
"Soumission","¬£49.79","One"
```

Open with: Excel, Google Sheets, Python (pandas), or any text editor

---

## ‚ö†Ô∏è Important Rules

### Before Scraping Any Website:

- ‚úÖ Check `robots.txt`: Visit `https://website.com/robots.txt`
- ‚úÖ Read Terms of Service - look for scraping policies
- ‚úÖ Check if they offer an API (better than scraping!)
- ‚úÖ Add delays between requests (`time.sleep(2)`)
- ‚úÖ Use appropriate User-Agent headers
- ‚úÖ Don't overload their servers
- ‚úÖ Respect rate limits

### Technical Considerations:

- **Simple sites (static HTML)**: Use BeautifulSoup (included)
- **Complex sites (JavaScript)**: Requires Selenium (`pip install selenium`)
- **Rate limiting**: Add `time.sleep(2)` between requests
- **Blocking issues**: Implement proxy rotation and User-Agent rotation

---

## üêõ Troubleshooting

### "No products found"
- Check selectors using browser inspector
- Verify the selector in console: `document.querySelectorAll('your-selector')`
- Make sure CSS selector syntax is correct

### "Connection error"
- Verify website is accessible in your browser
- Check internet connection
- Try increasing timeout: `WebScraper(url, timeout=30)`

### "ModuleNotFoundError: No module named 'bs4'"
- Install packages: `pip install -r requirements.txt`
- Or: `pip install beautifulsoup4 requests pandas`

### Website blocks the scraper
- Check their robots.txt and terms of service
- Add proper User-Agent header (already done by default)
- Implement delays between requests
- Consider their official API instead

---

## üìñ Example: Scraping Multiple Pages

```python
from scraper import WebScraper, CSVExporter
import time

all_products = []
base_url = "https://books.toscrape.com/catalogue/page-{}.html"

selectors = {
    'container': 'article.product_pod',
    'name': 'h3 a',
    'price': 'p.price_color',
    'rating': 'p.star-rating'
}

# Scrape pages 1-5
for page in range(1, 6):
    url = base_url.format(page)
    print(f"Scraping page {page}...")
    
    scraper = WebScraper(url)
    soup = scraper.fetch_page()
    products = scraper.parse_products(soup, selectors)
    all_products.extend(products)
    
    time.sleep(2)  # Be respectful - wait 2 seconds

# Export all products
CSVExporter.export(all_products, 'all_books.csv')
print(f"Scraped {len(all_products)} products!")
```

---

## üìö Learning Resources

### Topics to Understand

1. **HTML Structure**: Tags, classes, IDs
2. **CSS Selectors**: How to target elements
3. **BeautifulSoup**: Parsing HTML documents
4. **Requests Library**: Fetching web pages
5. **Pandas**: Working with CSV files

### Recommended Reading

- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [CSS Selectors Guide](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors)
- [Requests Library](https://requests.readthedocs.io/)
- [Pandas CSV Guide](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)

---

## üîß Advanced Features

### Custom CSV Export

```python
from scraper import CSVExporter

# Export with custom filename
CSVExporter.export(products, 'my_custom_file.csv')
```

### Error Handling

```python
try:
    csv_file = scrape_website(url, selectors)
except requests.RequestException as e:
    print(f"Network error: {e}")
except Exception as e:
    print(f"Scraping error: {e}")
```

### Checking Logs

All operations are logged to `scraper.log`:
```bash
# Windows
type scraper.log | more

# Mac/Linux
cat scraper.log
```

---

## ‚úÖ Checklist Before First Run

- [ ] Virtual environment created (`.venv` folder exists)
- [ ] Packages installed (`pip install -r requirements.txt`)
- [ ] Internet connection working
- [ ] Target website is accessible in browser
- [ ] CSS selectors tested in browser console
- [ ] robots.txt checked
- [ ] Terms of Service reviewed
- [ ] Appropriate delays planned for multi-page scraping

---

## üéì Learning Path

### Week 1: Basics
1. Run `quickstart.py` to see it working
2. Read `CONFIGURATION_GUIDE.py`
3. Practice finding selectors with browser inspector
4. Scrape 1-2 pages manually

### Week 2: Advancement
1. Implement multi-page scraping
2. Add error handling
3. Customize for your own website
4. Analyze and clean the CSV output

### Week 3: Mastery
1. Implement complex selector patterns
2. Add retry logic
3. Optimize performance
4. Handle edge cases

---

## üìû Need Help?

1. **Check Logs**: Review `scraper.log` for error details
2. **Test Selectors**: Use browser console (`F12`)
3. **Review Examples**: Look at `examples.py` and `website_configs.py`
4. **Read Documentation**: See `README.md` for full API reference
5. **Check robots.txt**: Ensure scraping is allowed

---

## üéâ Summary

You now have a complete web scraping toolkit! 

**Next step**: Run `python quickstart.py` to see it in action!

```bash
python quickstart.py
```

Happy Scraping! üöÄ

---

*Last Updated: January 2026*
*Python 3.8+, Windows/Mac/Linux Compatible*
